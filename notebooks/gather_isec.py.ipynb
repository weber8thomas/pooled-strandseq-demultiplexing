{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing as mp\n",
    "import parmap\n",
    "import sys, os\n",
    "import pandas as pd\n",
    "import gzip\n",
    "\n",
    "## List of inputs\n",
    "ref = pd.read_csv(snakemake.input.ref, compression=\"gzip\", sep=\"\\t\")\n",
    "# ref = pd.read_csv(\"/scratch/tweber/DATA/1000G_SNV_with_GT/OTF/OTF/BCFTOOLS_CONCAT_TAB/merge.txt.gz\", compression=\"gzip\", sep=\"\\t\")\n",
    "ref"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ref.ID.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ref.SAMPLE.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = mp.Manager()\n",
    "l_df_gb = m.list()\n",
    "l_df = m.list()\n",
    "\n",
    "def mp_vcf(vcf, l_df_gb, l_df):\n",
    "\n",
    "    # Nb of rows to skip at the beginning of the VCF\n",
    "    skip = len([e.decode(\"ISO-8859–1\").split('\\n') for e in gzip.open(\"{}\".format(vcf, \"rb\")) if e.decode(\"ISO-8859–1\").split('\\n')[0].startswith(\"##\")])\n",
    "    # chrom = vcf.split(\"/\")[-3]\n",
    "    sample = vcf.replace(\".vcf.gz\", \"\")\n",
    "    df = pd.read_csv(vcf, compression=\"gzip\", skiprows=skip, sep=\"\\t\")\n",
    "    df[\"#CHROM\"] = df[\"#CHROM\"].astype(str)\n",
    "    df[\"#CHROM\"] = df[\"#CHROM\"].str.replace(\"chr\", \"\")\n",
    "    df[\"ID\"] = df[\"#CHROM\"].astype(str) + \":\" + df[\"POS\"].astype(str) + \":\" + df[\"REF\"] + \":\" + df[\"ALT\"]\n",
    "\n",
    "    # df = df.loc[df[\"QUAL\"] >= 10]\n",
    "\n",
    "    merge_df = pd.merge(ref, df, on=\"ID\", how='right')\n",
    "\n",
    "    merge_df[\"SAMPLE\"] = merge_df[\"SAMPLE\"].fillna(\"NAN\")\n",
    "    merge_df[\"QUERY_SAMPLE_CELL\"] = sample\n",
    "    \n",
    "\n",
    "    merge_df_gb = merge_df.groupby(\"SAMPLE\")[\"ID\"].nunique().sort_values(ascending=False).reset_index()\n",
    "    merge_df_gb[\"QUERY_SAMPLE_CELL\"] = sample\n",
    "    merge_df_gb[\"Rank\"] = list(range(1,1+merge_df_gb.shape[0]))\n",
    "    merge_df_gb[\"Total_SNP\"] = df.shape[0]\n",
    "\n",
    "    l_df_gb.append(merge_df_gb)\n",
    "    l_df.append(merge_df)\n",
    "# f = snakemake.input.gt_sample_folder\n",
    "# f = \"/scratch/tweber/DATA/1000G_SNV_with_GT/OTF/GENOTYPING_OTF_POOL1/\"\n",
    "# f_ldir = sorted([e for e in os.listdir(f) if e.endswith(\".vcf.gz\")])\n",
    "f_ldir = sorted(list(snakemake.input.vcf))\n",
    "parmap.starmap(mp_vcf, list(zip(f_ldir)), l_df_gb, l_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.max_rows = 250\n",
    "concat_df = pd.concat(list(l_df))\n",
    "concat_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concat_counts_df = concat_df.loc[concat_df[\"QUAL\"] >= 10].groupby(\"QUERY_SAMPLE_CELL\")[\"SAMPLE\"].value_counts().rename(\"Counts\").reset_index()\n",
    "concat_counts_df = concat_df.groupby(\"QUERY_SAMPLE_CELL\")[\"SAMPLE\"].value_counts().rename(\"Counts\").reset_index()\n",
    "concat_counts_df[\"Rank\"] = concat_counts_df.groupby(\"QUERY_SAMPLE_CELL\")[\"SAMPLE\"].transform(lambda r: range(1, len(r) + 1))\n",
    "concat_counts_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coverage_df = pd.read_csv(snakemake.input.coverage, sep=\"\\t\")\n",
    "# coverage_df = pd.read_csv(\"/scratch/tweber/DATA/1000G_SNV_with_GT/OTF/COVERAGE/HGSVCxPool1/MERGE/merge_coverage.txt\", sep=\"\\t\")\n",
    "# coverage_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ashleys_predictions = pd.read_csv(snakemake.input.ashleys_predictions, sep=\"\\t\").rename({\"cell\": \"QUERY_SAMPLE_CELL\", \"prediction\" : \"ashleys_prediction\", \"probability\": \"ashleys_probability\"}, axis=1).sort_values(by=\"QUERY_SAMPLE_CELL\")\n",
    "# ashleys_predictions = pd.read_csv(\"/g/korbel2/weber/MosaiCatcher_files/POOLING/POOL1_RESEQ/HGSVCxPool1/cell_selection/labels.tsv\", sep=\"\\t\").rename({\"cell\": \"QUERY_SAMPLE_CELL\", \"prediction\" : \"ashleys_prediction\", \"probability\": \"ashleys_probability\"}, axis=1).sort_values(by=\"QUERY_SAMPLE_CELL\")\n",
    "ashleys_predictions[\"QUERY_SAMPLE_CELL\"] = ashleys_predictions[\"QUERY_SAMPLE_CELL\"].str.replace(\".sort.mdup.bam\", \"\")\n",
    "# mc_predictions = pd.read_csv(\"/g/korbel2/weber/MosaiCatcher_files/POOLING/POOL1_RESEQ/HGSVCxPool1/counts/HGSVCxPool1.info_raw\", skiprows=13, sep=\"\\t\").rename({\"cell\": \"QUERY_SAMPLE_CELL\"}, axis=1)[[\"QUERY_SAMPLE_CELL\", \"mapped\", \"good\", \"pass1\"]].rename({\"mapped\": \"reads_mapped\", \"good\" : \"reads_used\", \"pass1\": \"mc_coverage_compliant\"}, axis=1)\n",
    "mc_predictions = pd.read_csv(snakemake.input.mc_predictions, skiprows=13, sep=\"\\t\").rename({\"cell\": \"QUERY_SAMPLE_CELL\"}, axis=1)[[\"QUERY_SAMPLE_CELL\", \"mapped\", \"good\", \"pass1\"]].rename({\"mapped\": \"reads_mapped\", \"good\" : \"reads_used\", \"pass1\": \"mc_coverage_compliant\"}, axis=1)\n",
    "\n",
    "ashleys_mc_predictions = pd.merge(\n",
    "    ashleys_predictions,\n",
    "    mc_predictions,\n",
    "    on=\"QUERY_SAMPLE_CELL\"\n",
    ")\n",
    "ashleys_mc_predictions.loc[(ashleys_mc_predictions[\"ashleys_prediction\"] == 1) & (ashleys_mc_predictions[\"mc_coverage_compliant\"] == 1), \"Used/Not used in MC\"] = 1\n",
    "ashleys_mc_predictions[\"Used/Not used in MC\"] = ashleys_mc_predictions[\"Used/Not used in MC\"].fillna(0)\n",
    "ashleys_mc_predictions[\"Used/Not used in MC\"] = ashleys_mc_predictions[\"Used/Not used in MC\"].astype(int)\n",
    "ashleys_mc_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concat_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qual_cutoff = 10\n",
    "merge_df = pd.merge(\n",
    "        concat_df.groupby(\"QUERY_SAMPLE_CELL\")[\"ID\"].nunique().rename(\"QUAL >= 0\").reset_index(),\n",
    "        concat_df.loc[concat_df[\"QUAL\"] >= qual_cutoff].groupby(\"QUERY_SAMPLE_CELL\")[\"ID\"].nunique().rename(\"QUAL >= {}\".format(str(qual_cutoff))).reset_index(),\n",
    "        on=\"QUERY_SAMPLE_CELL\",how=\"outer\").sort_values(by=\"QUERY_SAMPLE_CELL\")\n",
    "\n",
    "import numpy as np\n",
    "vcf_list = list(concat_df.QUERY_SAMPLE_CELL.unique())\n",
    "empty_samples_list = [{\"QUERY_SAMPLE_CELL\" : \"HGSVCPool1x02PE20{}\".format(str(e)), \"QUAL >= 0\" : np.nan, \"QUAL >= {}\".format(str(qual_cutoff)) : np.nan, } for e in list(range(301, 397)) if \"HGSVCPool1x02PE20{}\".format(str(e)) not in vcf_list]\n",
    "\n",
    "merge_df = pd.concat([merge_df, pd.DataFrame(empty_samples_list)]).sort_values(by=\"QUERY_SAMPLE_CELL\")\n",
    "\n",
    "concat_counts_df = concat_df.groupby(\"QUERY_SAMPLE_CELL\")[\"SAMPLE\"].value_counts().rename(\"Counts\").reset_index()\n",
    "concat_counts_df[\"Rank\"] = concat_counts_df.groupby(\"QUERY_SAMPLE_CELL\")[\"SAMPLE\"].transform(lambda r: range(1, len(r) + 1))\n",
    "\n",
    "merge_df = pd.merge(\n",
    "        merge_df,\n",
    "        pd.pivot_table(concat_counts_df.loc[concat_counts_df[\"Rank\"] <= 3], index=\"QUERY_SAMPLE_CELL\", columns=[\"Rank\"], values=[\"SAMPLE\"], aggfunc=lambda x: ' '.join(x)),\n",
    "        on=\"QUERY_SAMPLE_CELL\",\n",
    "        how=\"left\"\n",
    ")\n",
    "merge_df = pd.merge(\n",
    "        merge_df,\n",
    "        pd.pivot_table(concat_counts_df.loc[concat_counts_df[\"Rank\"] <= 3], index=\"QUERY_SAMPLE_CELL\", columns=[\"Rank\"]),\n",
    "        on=\"QUERY_SAMPLE_CELL\",\n",
    "        how=\"left\"\n",
    ")\n",
    "\n",
    "concat_counts_df = concat_df.loc[concat_df[\"QUAL\"] >= qual_cutoff].groupby(\"QUERY_SAMPLE_CELL\")[\"SAMPLE\"].value_counts().rename(\"Counts\").reset_index()\n",
    "concat_counts_df[\"Rank\"] = concat_counts_df.groupby(\"QUERY_SAMPLE_CELL\")[\"SAMPLE\"].transform(lambda r: range(1, len(r) + 1))\n",
    "\n",
    "merge_df = pd.merge(\n",
    "        merge_df,\n",
    "        pd.pivot_table(concat_counts_df.loc[concat_counts_df[\"Rank\"] <= 3], index=\"QUERY_SAMPLE_CELL\", columns=[\"Rank\"], values=[\"SAMPLE\"], aggfunc=lambda x: ' '.join(x)),\n",
    "        on=\"QUERY_SAMPLE_CELL\",\n",
    "        how=\"left\"\n",
    ")\n",
    "merge_df = pd.merge(\n",
    "        merge_df,\n",
    "        pd.pivot_table(concat_counts_df.loc[concat_counts_df[\"Rank\"] <= 3], index=\"QUERY_SAMPLE_CELL\", columns=[\"Rank\"]),\n",
    "        on=\"QUERY_SAMPLE_CELL\",\n",
    "        how=\"left\"\n",
    ")\n",
    "merge_df = pd.merge(\n",
    "        merge_df,\n",
    "        coverage_df,\n",
    "        on=\"QUERY_SAMPLE_CELL\",\n",
    "        how=\"left\"\n",
    ")\n",
    "merge_df = pd.merge(\n",
    "        merge_df,\n",
    "        ashleys_mc_predictions,\n",
    "        on=\"QUERY_SAMPLE_CELL\",\n",
    "        how=\"left\"\n",
    ")\n",
    "\n",
    "pd.options.display.max_columns = 50\n",
    "merge_df.columns = ['QUERY_SAMPLE_CELL', 'QUAL>=0', 'QUAL>={}'.format(str(qual_cutoff)), 'QUAL>=0_SAMPLE_1', 'QUAL>=0_SAMPLE_2', 'QUAL>=0_SAMPLE_3', 'QUAL>=0_Counts_1', 'QUAL>=0_Counts_2', 'QUAL>=0_Counts_3',\n",
    "'QUAL>={}_SAMPLE_1'.format(str(qual_cutoff)), 'QUAL>={}_SAMPLE_2'.format(str(qual_cutoff)), 'QUAL>={}_SAMPLE_3'.format(str(qual_cutoff)), 'QUAL>={}_Counts_1'.format(str(qual_cutoff)), 'QUAL>={}_Counts_2'.format(str(qual_cutoff)), 'QUAL>={}_Counts_3'.format(str(qual_cutoff)),  'Average_Coverage', 'ashleys_prediction', 'ashleys_probability', 'reads_mapped', 'reads_used', 'mc_coverage_compliant', 'Used/Not used in MC']\n",
    "\n",
    "\n",
    "# merge_df = merge_df[\n",
    "#         [\n",
    "#              'QUERY_SAMPLE_CELL',   'QUAL>={}'.format(str(qual_cutoff)), 'QUAL>={}_SAMPLE_1'.format(str(qual_cutoff)), 'QUAL>={}_SAMPLE_2'.format(str(qual_cutoff)), 'QUAL>={}_SAMPLE_3'.format(str(qual_cutoff)), 'QUAL>={}_Counts_1'.format(str(qual_cutoff)), 'QUAL>={}_Counts_2'.format(str(qual_cutoff)), 'QUAL>={}_Counts_3'.format(str(qual_cutoff)),  'Average_Coverage', 'ashleys_prediction', 'ashleys_probability', 'reads_mapped', 'reads_used', 'mc_coverage_compliant', 'Used/Not used in MC'\n",
    "#         ]\n",
    "# ]\n",
    "merge_df[\"Average_Coverage\"] = merge_df[\"Average_Coverage\"]*100\n",
    "\n",
    "# merge_df = merge_df.loc[merge_df[\"mc_coverage_compliant\"] == 1].sort_values(by=\"Average_Coverage\")\n",
    "\n",
    "# merge_df.to_excel(\"/g/korbel2/weber/MosaiCatcher_files/POOLING/POOL1_RESEQ/HGSVCxPool1/config/stats_pooling.xlsx\", index=False)\n",
    "merge_df.to_excel(snakemake.output.stats, index=False)\n",
    "\n",
    "# import os, sys, stat\n",
    "# os.chmod(\"/g/korbel2/weber/MosaiCatcher_files/POOLING/POOL1_RESEQ/HGSVCxPool1/config/stats_pooling.xlsx\", stat.S_IRWXG)\n",
    "\n",
    "merge_df.sort_values(by=\"Average_Coverage\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge_df[['ashleys_prediction', 'mc_coverage_compliant', 'Used/Not used in MC']].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import seaborn as sns\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# from matplotlib.colors import LogNorm, Normalize\n",
    "\n",
    "\n",
    "# plt.style.use('default')\n",
    "# plt.figure(figsize=(10,20))\n",
    "\n",
    "\n",
    "# merge_df\n",
    "\n",
    "# ax = sns.barplot(data=merge_df.sort_values(by=\"QUERY_SAMPLE_CELL\"),  y=\"QUERY_SAMPLE_CELL\", x=\"Average_Coverage\", hue=\"mc_coverage_compliant\")\n",
    "# ax.set_xticklabels(ax.get_xticklabels(), rotation=90)\n",
    "# ax.axvline(0.34, ls=\"--\", color=\"grey\", lw=0.5)\n",
    "# ax.set_xlabel(\"Average Coverage %\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_df.columns"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "17a513ed38e3bc29591f8d3e8010ec62838828386159762dc70ad27982cd2bfe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
