import pandas as pd
import os, sys
import glob


output_dir = "/scratch/tweber/DATA/MC_DATA/DEMULTIPLEXING_POOLS/FINAL_RESULTS/without_sanity_check"


sample_raw = "2021-08-03-H22VWAFX3/HGSVCxpool2x02"




l_samples = [
# "2021-07-29-HWYJ2AFX2/HGSVCxpool1x01",
# "2021-08-03-H22VWAFX3/HGSVCxpool2x02",
# "2021-08-03-H22VWAFX3/HGSVCxpool3x01",
# "2023-11-09-HW5NFAFX5/HGSVCpool1NEW",
# "2023-11-09-HW3YVAFX5/LanexHGSVCpool2NEW",
# "2024-01-29-H33YJAFX7/LanexHGSVCpool2NEW96wellUVLED"
#"2024-01-30-AACT75KM5/HGSVCpool1NEWp2"
 #"2024-01-30-AACT75KM5/HGSVCpool3NEWp2"
  sample_raw  
]

sample = l_samples[0].split("/")[-1]
sample_for_savedir = l_samples[0].replace("/", "--")
print(sample_for_savedir)

#l_samples = [
#"2021-08-03-H22VWAFX3/HGSVCxpool3x01"
#"2021-08-03-H22VWAFX3/HGSVCxpool3x01"
#"2021-07-29-HWYJ2AFX2/HGSVCxpool1x01"
#"2023-11-09-HW3YVAFX5/LanexHGSVCpool2NEW"
#"2023-11-09-HW5NFAFX5/HGSVCpool1NEW"
#"2021-05-12-HMCNFAFX2/GM18989x01",
#"2021-04-21-HMNJLAFX2/GM19320x01",
#"2021-05-12-HMCNFAFX2/GM19331x01",
#"2021-05-31-HNLFLAFX2/GM19836x01",
#"2021-05-31-HNLFLAFX2/GM20355x01",
#"2021-05-06-HN2VJAFX2/GM21487x01",
#"2021-04-21-HMNJLAFX2/HG02282x03",
#"2021-05-06-HN2VJAFX2/HG02554x01",
#"2021-05-31-HNLFLAFX2/HG02666x02",
#"2021-04-21-HMNJLAFX2/HG02769x01",
#"2021-05-06-HN2VJAFX2/HG02953x01",
#"2021-05-12-HMCNFAFX2/HG03452x02",
#]

year = list(set([e.split("-")[0] for e in l_samples]))[0]
#index = "PE20" if year == "2021" or year == "2022" else "iTRU"
os.makedirs(f"{output_dir}/{sample_for_savedir}", exist_ok=True)


# Glob pattern to find .vcf files for the given sample
vcf_files = glob.glob(f"/scratch/tweber/DATA/MC_DATA/DEMULTIPLEXING_POOLS/SNP_DISCOVERY/{sample}/*.vcf")

# Initialize the index variable
index = None

# Check if the list of files is not empty
if vcf_files:
    # Retrieve the first file in the list
    first_file = vcf_files[0]
    
    # Check if "PE20" or "iTRU" is in the filename of the first file
    if "PE20" in first_file:
        index = "PE20"
    elif "iTRU" in first_file:
        index = "iTRU"

# Print the index value or a message if no index was found
if index:
    print(f"Index found: {index}")
else:
    print("No PE20 or iTRU index found in the first file.")


ref_path = f"/scratch/tweber/DATA/MC_DATA/DEMULTIPLEXING_POOLS/BCFTOOLS_CONCAT_TAB/{sample}/merge.txt.gz"
#ref_path = "/scratch/tweber/DATA/MC_DATA/DEMULTIPLEXING_POOLS/BCFTOOLS_CONCAT_TAB/HGSVCxpool3x01/merge.txt.gz"
# ref_path = "/scratch/tweber/DATA/MC_DATA/DEMULTIPLEXING_POOLS/BCFTOOLS_CONCAT_TAB/PSEUDOPOOL/merge.txt.gz"
ref = pd.read_csv(ref_path, compression="gzip", sep="\t",)
ref["GlobalSample"] = ref_path.split("/")[-2]

if "with_sanity_check" in output_dir:

    sanity_check_path = "/scratch/tweber/DATA/MC_DATA/DEMULTIPLEXING_POOLS/BCFTOOLS_CONCAT_TAB/Random_for_sanity_check/merge.txt.gz"
    sanity_check_ref = pd.read_csv(sanity_check_path, compression="gzip", sep="\t")
    sanity_check_ref["GlobalSample"] = sanity_check_path.split("/")[-2]

    ref = pd.concat([ref, sanity_check_ref])

ref["ID"] = "chr" + ref["ID"]
ref


ref['unique_sample_count'] = ref.groupby('ID')['SAMPLE'].transform('nunique')
ref = ref[ref['unique_sample_count'] == 1]
#ref


ref


ref.groupby(["GlobalSample", "SAMPLE"])["ID"].count()



ref_count = ref.groupby(["GlobalSample", "SAMPLE"])["ID"].count().reset_index()
ref_count.to_csv(f"{output_dir}/{sample_for_savedir}/reference_file_SNP_counts.tsv", sep="\t")
ref_count.head()



#sanity_check_ref.SAMPLE.unique()


import glob
final_vcf = list()
for vcf_input in glob.glob(f"/scratch/tweber/DATA/MC_DATA/DEMULTIPLEXING_POOLS/SNP_DISCOVERY/{sample}/*.vcf"):
    cell_line = vcf_input.split("/")[-1].split("_")[0].replace(".vcf", "")
    vcf = pd.read_csv(vcf_input, skiprows=255, sep="\t")

    vcf["cell_line"] = cell_line
#    print(vcf)
    vcf["ID"] = vcf["#CHROM"] + ":" + vcf["POS"].astype(str) + ":" + vcf["REF"]+ ":"  + vcf["ALT"]
#    print(vcf_input)
#    print(cell_line.split(f"{index}")[0])
    vcf = vcf.drop([cell_line.split(f"{index}")[0]], axis=1)
#    vcf = vcf.drop(["HGSVCpool2iTRUE5"], axis=1)

#    print(vcf)
    final_vcf.append(vcf)
final_vcf = pd.concat(final_vcf)
final_vcf.to_csv(f"{output_dir}/{sample_for_savedir}/full_set_of_SNPs_called.tsv.gz", sep="\t", compression="gzip")

final_vcf


final_vcf['unique_sample_count'] = final_vcf.groupby('ID')['cell_line'].transform('nunique')
#final_vcf = final_vcf[final_vcf['unique_sample_count'] == 1]
final_vcf


final_vcf.cell_line.nunique()


merge_df = pd.merge(final_vcf, ref, on="ID", how="inner")
merge_df.to_csv(f"{output_dir}/{sample_for_savedir}/join_ref_to_SNPs_called.tsv.gz", sep="\t", compression="gzip")
merge_df.head()


merge_df.cell_line.nunique()




pd.options.display.max_rows = 200
pd.options.display.max_columns = 200
mosaicatcher_stats = pd.concat([pd.read_csv("/scratch/tweber/DATA/MC_DATA/STOCKS/{}/counts/{}.info_raw".format(sample, sample.split("/")[-1]), sep="\t", skiprows=13) for sample in l_samples])
mosaicatcher_stats["cell"] = mosaicatcher_stats["cell"].str.replace(".sort.mdup.bam", "")
mosaicatcher_stats.head()




pd.options.display.max_rows = 200
pd.options.display.max_columns = 200
ashleys_labels = pd.concat([pd.read_csv(f"/scratch/tweber/DATA/MC_DATA/STOCKS/{sample}/cell_selection/labels.tsv", sep="\t") for sample in l_samples])
ashleys_labels["cell"] = ashleys_labels["cell"].str.replace(".sort.mdup.bam", "")
ashleys_labels.head()


combine_ashleys_mc_stats = pd.merge(ashleys_labels, mosaicatcher_stats, on=["sample", "cell"], how="inner")
combine_ashleys_mc_stats.head()


merge_df.cell_line.nunique()


gb_sample_count = merge_df.rename({"cell_line": "cell"}, axis=1).groupby(["cell", "GlobalSample", "SAMPLE"])["ID"].count().reset_index()
gb_sample_count_stats = gb_sample_count.rename(columns={"cell_line": "cell"})
gb_sample_count_stats = pd.merge(gb_sample_count_stats, combine_ashleys_mc_stats, on=["cell"], how="inner")
gb_sample_count_stats = gb_sample_count_stats.loc[gb_sample_count_stats["prediction"] == 1]
gb_sample_count_stats["cell"] = gb_sample_count_stats["cell"].apply(lambda r: r.split(index)[1])
gb_sample_count_stats.to_csv(f"{output_dir}/{sample_for_savedir}/groupby_sample_counts_with_stats.tsv.gz", sep="\t", compression="gzip")
gb_sample_count_stats.head()


gb_sample_count_stats.cell.nunique()


gb_sample_count_stats.cell.nunique()


# pd.options.display.max_columns = None
# pd.options.display.max_rows = None
pivot_table_stats = pd.pivot_table(gb_sample_count_stats, columns=["GlobalSample", "SAMPLE"], index="cell", values="ID").fillna(0)
import scipy.stats as stats
pivot_table_stats_zscore = pivot_table_stats.apply(lambda x: stats.zscore(x), axis=1)
pivot_table_stats_zscore.to_csv(f"{output_dir}/{sample_for_savedir}/pivot_table_zscore_norm.tsv.gz", sep="\t", compression="gzip")
#pivot_table_stats_zscore.index = [e.split(index)[1] for e in pivot_table_stats_zscore.index]
pivot_table_stats_zscore.head()


pivot_table_stats


import seaborn as sns
import matplotlib.pyplot as plt
sns.set_theme(style="whitegrid")
plt.figure(figsize=(40,20))
ax = sns.heatmap(pivot_table_stats_zscore, cmap="Reds", vmin=-1, vmax=6)
#ax.set_title("Matched SNP nb in PseudoPool (z-score adjusted)")
#ax.set_xlabel("Sample")
#ax.set_ylabel("Cell Line")
ax.figure.savefig(f"{output_dir}/{sample_for_savedir}/heatmap_zscore_cell_per_sample.png")
ax


#import seaborn as sns
#import matplotlib.pyplot as plt
#sns.set_theme(style="whitegrid")
#plt.figure(figsize=(40,20))
#ax = sns.clustermap(pivot_table_stats_zscore, cmap="Reds", vmin=-1, vmax=6)
##ax.set_title("Matched SNP nb in PseudoPool (z-score adjusted)")
##ax.set_xlabel("Sample")
##ax.set_ylabel("Cell Line")
#ax.figure.savefig(f"{output_dir}/{sample}/clustermap_zscore_cell_per_sample.png")
#ax


["-".join(e) for e in pivot_table_stats_zscore.columns]


import seaborn as sns
import matplotlib.pyplot as plt
sns.set_theme(style="whitegrid")
# Assuming pivot_table_stats_zscore is your data

# Create the clustermap
ax = sns.clustermap(pivot_table_stats_zscore, cmap="Reds", vmin=-1, vmax=6)

# Set the size of the figure
width, height = 40, 30  # You can adjust these values as needed
ax.fig.set_size_inches(width, height)
plt.setp(ax.ax_heatmap.get_xticklabels(), rotation=90, ha="right", rotation_mode="anchor", fontsize=10)  # Adjust fontsize as needed

# Get the number of labels (assuming they are the same as the number of columns in your data)
num_labels = pivot_table_stats_zscore.shape[1]

# Set the x-tick labels manually
ax.ax_heatmap.set_xticks([x+0.5 for x in range(num_labels)])
ax.ax_heatmap.set_xticklabels(["-".join(e) for e in pivot_table_stats_zscore.columns], rotation=90,  ha='right', rotation_mode='anchor', fontsize=16 )
print(ax.ax_heatmap.get_xticklabels())

# Save the figure
output_file = f"{output_dir}/{sample}/clustermap_zscore_cell_per_sample.png"
#ax.savefig(output_file)

#ax.set_title("Matched SNP nb in PseudoPool (z-score adjusted)")
#ax.set_xlabel("Sample")
#ax.set_ylabel("Cell Line")
ax.figure.savefig(f"{output_dir}/{sample_for_savedir}/clustermap_zscore_cell_per_sample.png")
ax


pivot_table_stats_zscore


pivot_table_stats_zscore_no_mi =  pivot_table_stats_zscore.copy()
pivot_table_stats_zscore_no_mi.columns = ["-".join(e) for e in pivot_table_stats_zscore.columns.tolist()]
pivot_table_stats_zscore_no_mi


melt_pivot_table_stats_zscore = pd.melt(
    pivot_table_stats_zscore_no_mi.reset_index(), id_vars=["cell"], value_vars=["-".join(e) for e in pivot_table_stats_zscore.columns.tolist()]
)


combine_ashleys_mc_stats["cell"] = combine_ashleys_mc_stats["cell"].apply(lambda r: r.split(index)[1])
combine_ashleys_mc_stats.head()


gb_sample_count_stats[["cell", "GlobalSample", "SAMPLE", "ID"]].rename(columns={"GlobalSample" : "Pool", "SAMPLE" : "1KG_identified_sample", "ID": "SNP_nb"}).head()


gb_sample_count_stats[["cell", "GlobalSample", "SAMPLE", "ID"]].rename(columns={"GlobalSample" : "Pool", "SAMPLE" : "1KG_identified_sample", "ID": "SNP_nb"}).head()


predictions_to_export = melt_pivot_table_stats_zscore.groupby(["cell"]).apply(lambda r: r.nlargest(1, 'value')).reset_index(drop=True)
predictions_to_export.loc[predictions_to_export["value"] < 5, "Trustable"] = False
predictions_to_export.loc[predictions_to_export["value"] >= 5, "Trustable"] = True
predictions_to_export[["GlobalSample", "SampleID"]] = predictions_to_export["variable"].str.split("-", expand=True)
predictions_to_export = predictions_to_export.rename(columns={"value": "z-score_value", "GlobalSample" : "Pool", "SampleID" : "1KG_identified_sample"}).drop("variable", axis=1)
predictions_to_export = pd.merge(predictions_to_export, gb_sample_count_stats[["cell", "GlobalSample", "SAMPLE", "ID"]].rename(columns={"GlobalSample" : "Pool", "SAMPLE" : "1KG_identified_sample", "ID": "SNP_nb"}), on=["cell", "Pool", "1KG_identified_sample"])
predictions_to_export = predictions_to_export[["cell", "Pool", "1KG_identified_sample", "z-score_value", "SNP_nb", "Trustable"]]
predictions_to_export.to_excel(f"{output_dir}/{sample_for_savedir}/{sample}_predictions_lite.xlsx", index=False)
predictions_to_export_lite = predictions_to_export.copy()
predictions_to_export = pd.merge(predictions_to_export, combine_ashleys_mc_stats, on="cell")
# Create tuples for MultiIndex
multiindex_columns = [('demultiplexing predictions', col) if col in predictions_to_export_lite.columns else ('metrics', col) for col in predictions_to_export.columns]
predictions_to_export_merge = predictions_to_export.copy()
# Assign MultiIndex to the columns of the merged DataFrame
predictions_to_export_merge.columns = pd.MultiIndex.from_tuples(multiindex_columns)

predictions_to_export_merge.set_index(("demultiplexing predictions", "cell")).to_excel(f"{output_dir}/{sample_for_savedir}/{sample}_predictions_with_metrics.xlsx", index=True)
predictions_to_export_merge



sns.boxplot(data=predictions_to_export, x="Trustable", y="good")



sns.boxplot(data=predictions_to_export, x="Trustable", y="dupl")


sns.boxplot(data=predictions_to_export, x="Trustable", y="probability")


sns.scatterplot(data=predictions_to_export, x="z-score_value", y="probability", hue="Trustable")


import matplotlib.pyplot as plt
import seaborn as sns


# Create a figure with 2x2 grid of subplots
fig, axs = plt.subplots(2, 2, figsize=(12, 10))

# Plotting
sns.boxplot(data=predictions_to_export, x="Trustable", y="good", ax=axs[0, 0])
sns.boxplot(data=predictions_to_export, x="Trustable", y="dupl", ax=axs[0, 1])
sns.boxplot(data=predictions_to_export, x="Trustable", y="probability", ax=axs[1, 0])
sns.scatterplot(data=predictions_to_export, x="z-score_value", y="probability", hue="Trustable", ax=axs[1, 1])

# Adjust layout for better presentation
plt.tight_layout()

# Save the figure
plt.savefig(f"{output_dir}/{sample_for_savedir}/combined_plots_with_metrics.png")

# Show the plot
plt.show()



pivot_table_stats_zscore_no_mi


melt_pivot_table_stats_zscore


melt_pivot_table_stats_zscore = pd.melt(
    pivot_table_stats_zscore_no_mi.reset_index(), id_vars=["cell"], value_vars=["-".join(e) for e in pivot_table_stats_zscore.columns.tolist()]
)

melt_pivot_table_stats_zscore = melt_pivot_table_stats_zscore.loc[~melt_pivot_table_stats_zscore["variable"].str.contains("Random")]
# melt_pivot_table_stats_zscore = melt_pivot_table_stats_zscore.loc[~melt_pivot_table_stats_zscore["variable"].str.contains("Random")].groupby(["cell"]).apply(lambda r: r.nlargest(1, 'value')).reset_index(drop=True)
# Assuming melt_pivot_table_stats_zscore is already defined and has the 'value' column.

# Calculate the numbers
total_points = len(melt_pivot_table_stats_zscore)
points_above_5 = melt_pivot_table_stats_zscore[(melt_pivot_table_stats_zscore['value'] > 5) & (~melt_pivot_table_stats_zscore["variable"].str.contains("Random"))].shape[0]
points_above_6 = melt_pivot_table_stats_zscore[(melt_pivot_table_stats_zscore['value'] > 6) & (~melt_pivot_table_stats_zscore["variable"].str.contains("Random"))].shape[0]
points_above_4 = melt_pivot_table_stats_zscore[(melt_pivot_table_stats_zscore['value'] > 4) & (~melt_pivot_table_stats_zscore["variable"].str.contains("Random"))].shape[0]

# Plotting
plt.figure(figsize=(40,20))
ax = sns.scatterplot(data=melt_pivot_table_stats_zscore, x="cell", y="value", hue="variable")
ax.set_xticklabels(labels=ax.get_xticklabels(), rotation=90)
plt.axhline(5, ls="--", lw=1.5, color="r")

# Add text box for the summary
textstr = '\n'.join((
    f'Total : {total_points}',
    f' > 4: {points_above_4}',
    f' > 5: {points_above_5}',
    f' > 6: {points_above_6}'))
props = dict(boxstyle='round', facecolor='grey', alpha=0.5)
ax.text(0.05, 0.95, textstr, transform=ax.transAxes, fontsize=18,
        verticalalignment='top', bbox=props)


ax.set_ylim(2,6.3)
ax.figure.savefig(f"{output_dir}/{sample_for_savedir}/zscore_distribution.png")

plt.show()




import dash_bio

f = dash_bio.Clustergram(
        data=pivot_table_stats_zscore.values,
        column_labels=pivot_table_stats_zscore.columns.tolist(),
        row_labels=pivot_table_stats_zscore.index.tolist(),
        color_threshold={
            'row': 150,
            'col': 700
        },
        height=1800,
        width=1700,
        color_map=['white', 'red'],
        display_range=[-1, 6],
    
    )


f.write_html(f"{output_dir}/{sample_for_savedir}/clustermap_zscore_cell_per_sample_interactive.html")
f


mask = [col for col in pivot_table_stats_zscore.columns if col[0] != 'Random_for_sanity_check']

# Apply the mask to filter out unwanted columns
pivot_table_stats_zscore[mask].head()


pivot_table_stats_zscore_wt_multiindex = pivot_table_stats_zscore[mask].copy()
pivot_table_stats_zscore_wt_multiindex.columns = pivot_table_stats_zscore[mask].columns.droplevel()
pivot_table_stats_zscore_wt_multiindex = pivot_table_stats_zscore_wt_multiindex.reset_index().rename({"index":"cell"}, axis=1)
pivot_table_stats_zscore_wt_multiindex.head()                   


#pivot_table_stats_zscore.melt()
melt_df = pd.melt(pivot_table_stats_zscore_wt_multiindex, id_vars=['cell'], value_vars=[e for e in pivot_table_stats_zscore_wt_multiindex.columns if e not in ["cell"]])
melt_df.head()


def top_n_samples(group, n=3):
    return group.sort_values(by='ID', ascending=False).head(n)


# tmp_debug = gb_sample_count_stats.loc[(gb_sample_count_stats["sample"] == "GM19836x01") & (gb_sample_count_stats["prediction"] == 1)].sort_values(by=["cell", "ID"], ascending=[True, False])
pd.options.display.max_rows = None
gb_sample_count_stats
tmp_debug = gb_sample_count_stats.groupby('cell').apply(top_n_samples, n=3).reset_index(drop=True)
tmp_debug = pd.merge(tmp_debug, melt_df, on=["cell", "SAMPLE"], how="inner")
tmp_debug[["cell", "SAMPLE", "ID", "value", "probability", "good"]]
tmp_debug.head(100)
tmp_debug.to_excel(f"{output_dir}/{sample_for_savedir}/debug.xlsx", index=False)

# # Generate a color for each unique cell
# unique_cells = tmp_debug['cell'].unique()
# colors = [f"background-color: rgb({200 + i*20 % 55}, {220 - (i*20) % 55}, {200 + (i*30) % 55})" for i in range(len(unique_cells))]
# color_map = dict(zip(unique_cells, colors))

# # Apply the colors
# def apply_row_colors(row):
#     return [color_map[row['cell']]] * len(row)

# styled_df = tmp_debug.drop(["bam"], axis=1).style.apply(apply_row_colors, axis=1)

# # Display the styled DataFrame in Jupyter Notebook
# styled_df






tmp_debug.SAMPLE.nunique()



# Rpy2

%load_ext rpy2.ipython 



metadata = pd.read_csv("../20130606_g1k_3202_samples_ped_population.txt", sep=" ")
metadata.head()


melt_pivot_table_stats_zscore.head()
melt_pivot_table_stats_zscore[["GlobalSample", "SampleID"]] = melt_pivot_table_stats_zscore.variable.str.split("-", expand=True)
melt_pivot_table_stats_zscore.head()


merge_melt_pivot_table_stats_zscore = pd.merge(melt_pivot_table_stats_zscore, metadata, on="SampleID")
merge_melt_pivot_table_stats_zscore


merge_melt_pivot_table_stats_zscore.SampleID.nunique()


merge_melt_pivot_table_stats_zscore.groupby(["Superpopulation", "SampleID"])["cell"].nunique().reset_index()


import yaml
file_path = "pools_composition.yaml"
reference_sample_list = yaml.safe_load(open(file_path, "r"))


reference_sample_list.keys()


if not "NEW" in sample:
    corresponding_sample = [e for e in reference_sample_list if e in sample and "NEW" not in e][0]
else:
    corresponding_sample = [e for e in reference_sample_list if e in sample and "NEW" in e][0]

print(corresponding_sample)
reference_samples_with_metadata = pd.merge(
    pd.DataFrame(reference_sample_list[corresponding_sample], columns=[corresponding_sample]).melt().rename(columns={"variable":"Pool", "value":"SampleID"}),
    metadata
)
reference_samples_with_metadata


import matplotlib.pyplot as plt
import pandas as pd

# Assuming merge_melt_pivot_table_stats_zscore is a DataFrame that contains the data you've shown.

df = merge_melt_pivot_table_stats_zscore.groupby(["Superpopulation", "SampleID"])["cell"].nunique().reset_index()


# First, find all unique SampleIDs from reference_samples_with_metadata
unique_ref_sample_ids = reference_samples_with_metadata['SampleID'].unique()

# Identify missing SampleIDs in df
missing_sample_ids = set(unique_ref_sample_ids) - set(df['SampleID'].unique())

# For each missing SampleID, find its Superpopulation from reference_samples_with_metadata and add a row to df
for sample_id in missing_sample_ids:
    superpop = reference_samples_with_metadata.loc[reference_samples_with_metadata['SampleID'] == sample_id, 'Superpopulation'].values[0]
    # Add a new row to df for the missing SampleID
    # Assuming 'cell' count should be 0 (or another placeholder value) for missing SampleID
    df = pd.concat([df, pd.DataFrame({'Superpopulation': [superpop], 'SampleID': [sample_id], 'cell': [0]})], ignore_index=True)


# Unique superpopulations for color mapping
unique_superpopulations = df['Superpopulation'].unique()

# Generate a list of colors
colors = plt.cm.tab20.colors[:len(unique_superpopulations)]

# Create a color dictionary
color_dict = dict(zip(unique_superpopulations, colors))

# Add a color column to the dataframe
df['color'] = df['Superpopulation'].map(color_dict)

df = df.sort_values(by=["Superpopulation", "SampleID"])


df.to_csv(f"{output_dir}/{sample_for_savedir}/cell_count_by_sample_and_superpop_DATA.csv", index=False, sep="\t")

# Plot
plt.figure(figsize=(10, 8))
for _, row in df.iterrows():
    plt.bar(row['SampleID'], row['cell'], color=row['color'], label=row['Superpopulation'])

# Create a custom legend
from matplotlib.lines import Line2D
legend_elements = [Line2D([0], [0], color=color_dict[sp], lw=4, label=sp) for sp in unique_superpopulations]
plt.legend(handles=legend_elements, title='Superpopulation')

plt.xlabel('SampleID')
plt.ylabel('Cell count')
plt.xticks(rotation=90)  # Rotate the x labels so they don't overlap
plt.title('Cell Count by SampleID and Superpopulation')
plt.tight_layout()  # Adjust the plot to ensure everything fits without overlap
plt.savefig(f"{output_dir}/{sample_for_savedir}/cell_count_by_sample_and_superpop.png")

plt.show()



merge_melt_pivot_table_stats_zscore.groupby("Superpopulation")["SampleID"].nunique().reset_index()


pd.merge(merge_melt_pivot_table_stats_zscore["SampleID"].drop_duplicates().reset_index(), merge_melt_pivot_table_stats_zscore, on="SampleID").Superpopulation.value_counts()


import seaborn as sns
import matplotlib.pyplot as plt

# Assuming merge_melt_pivot_table_stats_zscore is your DataFrame
grouped_data = merge_melt_pivot_table_stats_zscore.groupby("Superpopulation")["SampleID"].nunique().reset_index()

# Unique superpopulations for color mapping
unique_superpopulations = grouped_data['Superpopulation'].unique()

# Generate a list of colors
colors = plt.cm.tab20.colors[:len(unique_superpopulations)]

# Create a color dictionary
color_dict = dict(zip(unique_superpopulations, colors))

# Create a color palette that Seaborn can use
palette = [color_dict[sp] for sp in grouped_data['Superpopulation']]


# Plot expected counts (background bars)
sns.barplot(data=reference_samples_with_metadata.Superpopulation.value_counts().reset_index(), x="Superpopulation", y="count", palette=palette, alpha=0.3, label='Expected Count')


df.to_csv(f"{output_dir}/{sample_for_savedir}/sample_by_superpop_recalled_DATA_background.csv", index=False, sep="\t")


# Plot
sns.barplot(data=grouped_data, x="Superpopulation", y="SampleID", palette=palette)

plt.xlabel('Superpopulation')
plt.ylabel('Unique Sample IDs')
plt.title('Unique Sample IDs per Superpopulation')
plt.savefig(f"{output_dir}/{sample_for_savedir}/sample_by_superpop_recalled.png")

plt.show()



sns.barplot(data=merge_melt_pivot_table_stats_zscore.Population.value_counts().reset_index(), x="Population", y="count")


merge_melt_pivot_table_stats_zscore.loc[merge_melt_pivot_table_stats_zscore["FatherID"] != "0"].shape[0]


pivot_table_stats_zscore.head()


pivot_table_stats_zscore_wt_multiindex = pivot_table_stats_zscore_wt_multiindex.set_index("cell")
pivot_table_stats_zscore_wt_multiindex.head()


%%R -i output_dir -i sample_for_savedir -i pivot_table_stats_zscore_wt_multiindex -i metadata -i ref_count -w 1800 -h 1500
library(ComplexHeatmap)
library(circlize)

set.seed(123) # for reproducibility
ordered_metadata <- metadata[match(colnames(pivot_table_stats_zscore_wt_multiindex), metadata$SampleID), ]

# Map GlobalSample to the SAMPLE in pivot_table_stats_zscore
global_sample_annotation <- ref_count[match(colnames(pivot_table_stats_zscore_wt_multiindex), ref_count$SAMPLE), "GlobalSample"]


# Hex color codes provided by you
superpopulation_colors_hex <- c(
  "AFR" = "#3274A1",
  "AMR" = "#E1812C",
  "EAS" = "#EEBB89",
  "EUR" = "#B5C8E1",
  "SAS" = "#3A923A"
)

# Assuming ordered_metadata is your metadata dataframe which has a column 'Superpopulation'

# Check if all superpopulations in your data are covered by the colors defined
if(!all(unique(ordered_metadata$Superpopulation) %in% names(superpopulation_colors_hex))) {
  stop("Not all superpopulations have a defined color")
}

# Create HeatmapAnnotation objects with the new colors
col_annotation <- HeatmapAnnotation(
  df = ordered_metadata[c("Population", "Superpopulation")],
  col = list(
    Superpopulation = superpopulation_colors_hex  # Apply the color mapping
    # Add other annotations if necessary
  ),
  GlobalSample = global_sample_annotation  # Assuming global_sample_annotation is previously defined
)

# Convert the pandas DataFrame to an R matrix
mat <- as.matrix(pivot_table_stats_zscore_wt_multiindex)
#print(head(mat))

file_path <- paste(output_dir, sample_for_savedir, "clustermap_zscore_cell_per_sample_with_annotations.png", sep="/")
png(file_path, width = 1800, height = 1500)

# Creating the heatmap
Heatmap(mat, 
        name = "z-score", 
        col = colorRamp2(c(-1, 6), c("white", "red")),
        top_annotation = col_annotation,
        cluster_rows = TRUE, 
        cluster_columns = TRUE,
        show_row_names = TRUE,
        show_column_names = TRUE,
       row_names_gp = gpar(fontsize = 8)) # Adjust fontsize as needed

#dev.off()














#%%R -i pivot_table_stats_zscore_wt_multiindex -i metadata -i ref_count -w 1800 -h 1500
#
## Ensure that the InteractiveComplexHeatmap package is installed
#if (!requireNamespace("InteractiveComplexHeatmap", quietly = TRUE)) {
#    if (!requireNamespace("BiocManager", quietly = TRUE)) {
#        install.packages("BiocManager")
#    }
#    BiocManager::install("InteractiveComplexHeatmap")
#}
#
#library(ComplexHeatmap)
#library(InteractiveComplexHeatmap)
#
## Assuming pivot_table_stats_zscore_wt_multiindex is loaded in your R environment
## as well as the metadata and ref_count
#
## Your existing code for the heatmap setup
#ordered_metadata <- metadata[match(colnames(pivot_table_stats_zscore_wt_multiindex), metadata$SampleID), ]
#global_sample_annotation <- ref_count[match(colnames(pivot_table_stats_zscore_wt_multiindex), ref_count$SAMPLE), "GlobalSample"]
#col_annotation <- HeatmapAnnotation(df = ordered_metadata[c("Population", "Superpopulation")],
#                                    GlobalSample = global_sample_annotation)
#mat <- as.matrix(pivot_table_stats_zscore_wt_multiindex)
#
## Your ComplexHeatmap creation
#ht = Heatmap(mat, 
#             name = "z-score", 
#             col = colorRamp2(c(-1, 6), c("white", "red")),
#             top_annotation = col_annotation,
#             cluster_rows = TRUE, 
#             cluster_columns = TRUE,
#             show_row_names = TRUE,
#             show_column_names = TRUE,
#             row_names_gp = gpar(fontsize = 8)) # Adjust fontsize as needed
#
## Now to make it interactive
#ht = draw(ht, heatmap_legend_side = "bot", annotation_legend_side = "bot")
#interactive_heatmap(ht)
#


# ashleys_labels.loc[(ashleys_labels["sample"] == "GM19836x01") & (ashleys_labels["prediction"] == 1)]


#pivot_table_stats_zscore.columns = pivot_table_stats_zscore.columns.droplevel()
#pivot_table_stats_zscore_melt = pivot_table_stats_zscore.melt(
#    ignore_index=False,
#    var_name="SAMPLE",
#    value_name="SNP nb (z-score adjusted)",
#).reset_index()   
#pivot_table_stats_zscore_melt["Sample_to_find"] = pivot_table_stats_zscore_melt["cell"].apply(lambda r: r.split("x")[0].replace("GM", "NA"))
## retrieve the Sample with highest SNP nb
#pivot_table_stats_zscore_melt = pivot_table_stats_zscore_melt.loc[pivot_table_stats_zscore_melt.groupby("cell")["SNP nb (z-score adjusted)"].idxmax()]
##pivot_table_stats_zscore_melt.loc[pivot_table_stats_zscore_melt["Sample_to_find"] == pivot_table_stats_zscore_melt["SAMPLE"], "Match"] = True
##pivot_table_stats_zscore_melt.loc[pivot_table_stats_zscore_melt["Sample_to_find"] != pivot_table_stats_zscore_melt["SAMPLE"], "Match"] = False
#pivot_table_stats_zscore_melt


#false_match = pivot_table_stats_zscore_melt.loc[pivot_table_stats_zscore_melt["Match"] == False]
#false_match = pd.merge(false_match, gb_sample_count_stats, on=["cell", "SAMPLE"], how="inner")
#false_match


#false_match.cell.values





#pd.options.display.max_rows = 210
#tmp_false = gb_sample_count_stats.loc[gb_sample_count_stats["cell"].isin(false_match.cell.values.tolist())].sort_values(by=["cell", "ID"], ascending=[True, False])
#
#def top_n_samples(group, n=3):
#    return group.sort_values(by='ID', ascending=False).head(n)
#
#tmp_false = tmp_false.groupby('cell').apply(top_n_samples, n=3).reset_index(drop=True)
#
#
#def highlight_cells(x):
#    colors = ['background-color: yellow', 'background-color: lightgreen', 'background-color: lightblue']
#    return [colors[i % len(colors)] if x.name in top_samples['cell'].unique() else '' for i in range(len(x))]
#
## Generate a color for each unique cell
#unique_cells = tmp_false['cell'].unique()
#colors = [f"background-color: rgb({200 + i*20 % 55}, {220 - (i*20) % 55}, {200 + (i*30) % 55})" for i in range(len(unique_cells))]
#color_map = dict(zip(unique_cells, colors))
#
## Apply the colors
#def apply_row_colors(row):
#    return [color_map[row['cell']]] * len(row)
#
#styled_df = tmp_false.drop(["bam"], axis=1).style.apply(apply_row_colors, axis=1)
#
## Display the styled DataFrame in Jupyter Notebook
#styled_df
